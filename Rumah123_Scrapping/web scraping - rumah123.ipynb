{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from seleniumbase import Driver\n",
    "import csv\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.rumah123.com/jual/tangerang/residensial/?page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links_from_page(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "    card_containers = my_html.find_all('div', class_='card-featured__middle-section')\n",
    "\n",
    "    nav_links = []\n",
    "    for container in card_containers:\n",
    "        property_link = container.find('a', href=True)\n",
    "        href = property_link['href'] if property_link else \"unknown\"\n",
    "        nav_links.append(f'https://www.rumah123.com{href}')\n",
    "    \n",
    "    return nav_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_property_details(driver, link):\n",
    "    driver.get(link)\n",
    "    time.sleep(5)  \n",
    "    details = {'url': link}\n",
    "    \n",
    "    try:\n",
    "        header_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'ui-container.ui-property-page__main-container'))\n",
    "        )\n",
    "        soup_header = BeautifulSoup(header_element.get_attribute('innerHTML'), 'html.parser')\n",
    "\n",
    "        # Extract data\n",
    "        price_text = soup_header.select_one('.r123-listing-summary__price').text\n",
    "        try:\n",
    "            currency, price, price_unit = price_text.split()\n",
    "        except ValueError:\n",
    "            currency, price, price_unit = \"unknown\", \"unknown\", \"unknown\"\n",
    "        \n",
    "        title = soup_header.select_one('.r123-listing-summary__header-container-title').text.strip()\n",
    "        address = soup_header.select_one('.r123-listing-summary__header-container-address').text.strip()\n",
    "        \n",
    "        # Compile header information\n",
    "        details.update({\n",
    "            'title': title,\n",
    "            'price_currency': currency,\n",
    "            'price_value': price,\n",
    "            'price_unit': price_unit,\n",
    "            'address': address\n",
    "        })\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Scrape the specifications\n",
    "        specifications_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'listing-specification-v2__content'))\n",
    "        )\n",
    "        soup_specifications = BeautifulSoup(specifications_element.get_attribute('innerHTML'), 'html.parser')\n",
    "\n",
    "        items = soup_specifications.select('.listing-specification-v2__item')\n",
    "        for item in items:\n",
    "            label = item.find(class_='listing-specification-v2__item-label').text.strip()\n",
    "            value = item.find(class_='listing-specification-v2__item-value').text.strip()\n",
    "            details[label] = value\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Extract lat and lon from the script tag\n",
    "        script_tag = driver.find_element(By.ID, '99-root-props')\n",
    "        script_content = script_tag.get_attribute('innerHTML')\n",
    "        script_json = json.loads(script_content)\n",
    "        \n",
    "        location = script_json.get('property', {}).get('data', {}).get('gallery', {}).get('map', {}).get('location', {})\n",
    "        lat = location.get('lat', 'unknown')\n",
    "        lon = location.get('lng', 'unknown')\n",
    "        \n",
    "        details.update({\n",
    "            'latitude': lat,\n",
    "            'longitude': lon\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting lat/lon: {e}\")\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: uc_driver not found. Getting it now:\n",
      "\n",
      "*** chromedriver to download = 125.0.6422.141 (Latest Stable) \n",
      "\n",
      "Downloading chromedriver-mac-x64.zip from:\n",
      "https://storage.googleapis.com/chrome-for-testing-public/125.0.6422.141/mac-x64/chromedriver-mac-x64.zip ...\n",
      "Download Complete!\n",
      "\n",
      "Extracting ['chromedriver'] from chromedriver-mac-x64.zip ...\n",
      "Unzip Complete!\n",
      "\n",
      "The file [uc_driver] was saved to:\n",
      "/Users/salmadanu/Desktop/Bangkit/HouseSpot/HouseSpot-ML/.venv/lib/python3.9/site-packages/seleniumbase/drivers/uc_driver\n",
      "\n",
      "Making [uc_driver 125.0.6422.141] executable ...\n",
      "[uc_driver 125.0.6422.141] is now ready for use!\n",
      "\n",
      "Data has been written to tangerang-2.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize the WebDriver and scrape links from multiple pages\n",
    "all_nav_links = []\n",
    "for page_num in range(21, 22):  # Adjust the range for the number of pages to scrape\n",
    "    driver = initialize_driver(page_num)\n",
    "    page_links = scrape_links_from_page(driver)\n",
    "    all_nav_links.extend(page_links)\n",
    "    driver.quit()\n",
    "\n",
    "# Scrape property details from each link\n",
    "all_data = []\n",
    "for link in all_nav_links:\n",
    "    driver = Driver(uc=True)\n",
    "    details = scrape_property_details(driver, link)\n",
    "    all_data.append(details)\n",
    "    driver.quit()\n",
    "\n",
    "# Determine the header for the CSV file (all unique keys from the scraped data)\n",
    "header = set()\n",
    "for data in all_data:\n",
    "    header.update(data.keys())\n",
    "\n",
    "header = sorted(header)\n",
    "\n",
    "# Write data to CSV file\n",
    "with open('/Users/salmadanu/Desktop/Bangkit/HouseSpot/HouseSpot-ML/Rumah123_Scrapping/CSV_Files/tangerang-2.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for data in all_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(\"Data has been written to tangerang-2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
